# Architecture Concept

The OpenVINO&trade; model server is a Python* implementation of the gRPC and RESTful API interfaces defined by TensorFlow* serving.
The OpenVINO model server backend uses the Inference Engine libraries from the OpenVINO&trade; toolkit, which speeds CPU execution
and enables it on FPGA and Movidius devices. 

The gRPC code skeleton is based on the proto-buffer API definition and generated by the protoc tool. 
Data serialization is aligned with the API definition implemented using Tensorflow libraries. 
Request handling is implemented through the set of Python classes that manages AI models in Intermediate Representation 
format and Inference Engine component which executes the graphs operations.


**Figure 1: Docker Container (VM or Bare Metal Host)**
![architecture chart](serving.png)

The OpenVINO model server requires the models to be present in the local file system or they could be hosted 
remotely on object storage services. Google* Cloud Storage and S3 compatible storage are supported. 

You can host the OpenVINO model server on a local server, a virtual machine, or in a docker container. It is also suitable for landing in a Kubernetes environment. 

The only two exposed network interfaces are gRPC and RESTful API, which currently _does not_ include authorization, 
authentication, or data encryption. Those functions are expected to be implemented outside of the model server 
(for example via Kubernetes* ingress or nginx forwarding proxy). 
Alternatively, they could be added to gRPC interface inside the OpenVINO&trade; Model Server but for now this is out of scope.
