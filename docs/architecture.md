# Architecture

The OpenVINO&trade; model server is a Python* implementation of the gRPC and RESTful API interfaces defined by TensorFlow* serving. These interfaces don't include authorization, authentication, or data encryption functions. You can implement these functions outside of the model server, such as through Kubernetes* ingress or the nginx forwarding proxy. As an option, you can add the functions to the gRPC interface in the OpenVINO model server. This optoin is out of scope for this documentation.

The back end of the OpenVINO model server uses the Inference Engine libraries from the OpenVINO&trade; toolkit, which speeds CPU execution and enables rhw OpenVINO model server on FPGA and IntelÂ® Movidius&trade; devices. 

The gRPC code skeleton is based on the proto-buffer API definition and is generated by the protoc tool. Data serialization is aligned with the API definition implemented by Tensorflow libraries. Request handling is implemented through the set of Python classes that manages AI models in Intermediate Representation format and the Inference Engine component that executes graph operations.


![architecture chart](serving.png)

**Figure 1: Docker Container (VM or Bare Metal Host)**

You can host the OpenVINO model server on:
- Local server.
- Virtual machine.
- Docker container.
- Kubernetes environment.

You can put your models on: 
- Local file system.
- Hosted remotely on object storage services. 
- On Google* Cloud Storage or on S3-compatible storage. 
